# Use an official openjdk runtime as a parent image
FROM openjdk:11

# Install wget and curl (if needed)
RUN apt-get update && apt-get install -y wget curl net-tools

# Set environment variables
ENV SPARK_VERSION=3.3.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/spark

# Download and extract Spark
RUN wget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar xvf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Set working directory
WORKDIR ${SPARK_HOME}

# Expose worker Web UI and ports
EXPOSE 8081 7078

# Copy start-worker script
COPY start-worker.sh /start-worker.sh
RUN chmod +x /start-worker.sh

# Default command to run Spark worker
CMD ["/start-worker.sh"]
